{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Download & install ollama**\n",
        "\n"
      ],
      "metadata": {
        "id": "FBzvUsO39AHn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCCdbnqrtGH3"
      },
      "outputs": [],
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install docker**"
      ],
      "metadata": {
        "id": "RXzY4PpQ9KQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !apt-get -qq install docker.io"
      ],
      "metadata": {
        "id": "Gj0xYlwiuQwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install udocker tool to run docker without root.**"
      ],
      "metadata": {
        "id": "gMUZLPW09YUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install udocker"
      ],
      "metadata": {
        "id": "TkJ-cxTjuk8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing localtunnel to expose ollama & cheshire cat AI ports.**"
      ],
      "metadata": {
        "id": "--GmKngx9dyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm i -g localtunnel"
      ],
      "metadata": {
        "id": "OD3Vu3LqvmKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting ipv4 ( this will be require to enter when we are opening local tunnel endpoints for ollama & cheshire cat ai )**"
      ],
      "metadata": {
        "id": "NBXFPxxj9mJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "g51HtXT-t7Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally! Running ollama server & Cheshire Cat togather with localtunnel, This will start ollama server & Cheshire cat AI tool togather. After running this open up the ollama & cheshire cat urls and enter above shown ip address. Then pull any model in ollama server ( for ref. https://github.com/jmorganca/ollama/blob/main/docs/api.md#request-18 ), then configure it in cheshire cat settings by adding endpoint & model name."
      ],
      "metadata": {
        "id": "zTh_UbOa93cO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lt --port 11434 & lt --port 1865 & OLLAMA_ORIGINS=* ollama serve & udocker --allow-root run -p 1865:80 ghcr.io/cheshire-cat-ai/core:latest"
      ],
      "metadata": {
        "id": "OkLFNJrMuy1r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}